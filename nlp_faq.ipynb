{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Question Answering from FAQ using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            Questions  Answers\n",
       "0   How to identify if a cluster is unhealthy, wha...        1\n",
       "1   What happens to the quotas when there is a bro...        2\n",
       "2   Document the process to open firewall from clo...        3\n",
       "3   How to add wildcard access to a specific topic...        4\n",
       "4                                 How is this billed?        5\n",
       "5          Can I change the retention of my pipeline?        6\n",
       "6   Why is creating a pipeline with topic Affinity...        7\n",
       "7                         Can I scale up my pipeline?        8\n",
       "8           Can I delete a single topic on a cluster?        9\n",
       "9   What is the difference between a pipeline, a t...       10\n",
       "10  Does your service support multiple topics on a...       11"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Questions</th>\n      <th>Answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How to identify if a cluster is unhealthy, wha...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What happens to the quotas when there is a bro...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Document the process to open firewall from clo...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How to add wildcard access to a specific topic...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How is this billed?</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Can I change the retention of my pipeline?</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Why is creating a pipeline with topic Affinity...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Can I scale up my pipeline?</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Can I delete a single topic on a cluster?</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>What is the difference between a pipeline, a t...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Does your service support multiple topics on a...</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "import pandas as pd;\n",
    "\n",
    "#Load dataset and examine dataset, rename columns to questions and answers\n",
    "\n",
    "df=pd.read_csv(\"KafkaFAQ.csv\");\n",
    "df.columns=[\"Questions\",\"Answers\"];\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['identify cluster unhealthy steps metrics look', 'happens quotas broker failure', 'document process open firewall cloud onprem vice versa', 'add wildcard access specific topic cluster', 'billed', 'change retention pipeline', 'creating pipeline topic affinity taking long', 'scale pipeline', 'delete single topic cluster', 'difference pipeline topic broker partition cluster', 'service support multiple topics single kafka cluster']\n\n\n['how to identify if a cluster is unhealthy what are the first steps or metrics we should look for', 'what happens to the quotas when there is a broker failure', 'document the process to open firewall from cloud to onprem or vice versa', 'how to add wildcard access to a specific topic in a cluster', 'how is this billed', 'can i change the retention of my pipeline', 'why is creating a pipeline with topic affinity taking so long', 'can i scale up my pipeline', 'can i delete a single topic on a cluster', 'what is the difference between a pipeline a topic a broker a partition and a cluster', 'does your service support multiple topics on a single kafka cluster']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import gensim \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "#st = LancasterStemmer()\n",
    "\n",
    "def clean_sentence(sentence, stopwords=False):\n",
    "    \n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    #sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "    \n",
    "    if stopwords:\n",
    "         sentence = remove_stopwords(sentence)\n",
    "    \n",
    "    #sent_stemmed='';\n",
    "    #for word in sentence.split():\n",
    "    #    sent_stemmed+=' '+st.stem(word) \n",
    "    #sentence=sent_stemmed\n",
    "    \n",
    "    return sentence\n",
    "                    \n",
    "def get_cleaned_sentences(df,stopwords=False):    \n",
    "    sents=df[[\"Questions\"]];\n",
    "    cleaned_sentences=[]\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        #print(index,row)\n",
    "        cleaned=clean_sentence(row[\"Questions\"],stopwords);\n",
    "        cleaned_sentences.append(cleaned);\n",
    "    return cleaned_sentences;\n",
    "\n",
    "cleaned_sentences=get_cleaned_sentences(df,stopwords=True)\n",
    "print(cleaned_sentences);\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "cleaned_sentences_with_stopwords=get_cleaned_sentences(df,stopwords=False)\n",
    "print(cleaned_sentences_with_stopwords);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0  :  a\n1  :  are\n2  :  cluster\n3  :  first\n4  :  for\n5  :  how\n6  :  identify\n7  :  if\n8  :  is\n9  :  look\n10  :  metrics\n11  :  or\n12  :  should\n13  :  steps\n14  :  the\n15  :  to\n16  :  unhealthy\n17  :  we\n18  :  what\n19  :  broker\n20  :  failure\n21  :  happens\n22  :  quotas\n23  :  there\n24  :  when\n25  :  cloud\n26  :  document\n27  :  firewall\n28  :  from\n29  :  onprem\n30  :  open\n31  :  process\n32  :  versa\n33  :  vice\n34  :  access\n35  :  add\n36  :  in\n37  :  specific\n38  :  topic\n39  :  wildcard\n40  :  billed\n41  :  this\n42  :  can\n43  :  change\n44  :  i\n45  :  my\n46  :  of\n47  :  pipeline\n48  :  retention\n49  :  affinity\n50  :  creating\n51  :  long\n52  :  so\n53  :  taking\n54  :  why\n55  :  with\n56  :  scale\n57  :  up\n58  :  delete\n59  :  on\n60  :  single\n61  :  and\n62  :  between\n63  :  difference\n64  :  partition\n65  :  does\n66  :  kafka\n67  :  multiple\n68  :  service\n69  :  support\n70  :  topics\n71  :  your\nhow to identify if a cluster is unhealthy what are the first steps or metrics we should look for\n[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]\nwhat happens to the quotas when there is a broker failure\n[(0, 1), (8, 1), (14, 1), (15, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]\ndocument the process to open firewall from cloud to onprem or vice versa\n[(11, 1), (14, 1), (15, 2), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)]\nhow to add wildcard access to a specific topic in a cluster\n[(0, 2), (2, 1), (5, 1), (15, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1)]\nhow is this billed\n[(5, 1), (8, 1), (40, 1), (41, 1)]\ncan i change the retention of my pipeline\n[(14, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1)]\nwhy is creating a pipeline with topic affinity taking so long\n[(0, 1), (8, 1), (38, 1), (47, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1)]\ncan i scale up my pipeline\n[(42, 1), (44, 1), (45, 1), (47, 1), (56, 1), (57, 1)]\ncan i delete a single topic on a cluster\n[(0, 2), (2, 1), (38, 1), (42, 1), (44, 1), (58, 1), (59, 1), (60, 1)]\nwhat is the difference between a pipeline a topic a broker a partition and a cluster\n[(0, 5), (2, 1), (8, 1), (14, 1), (18, 1), (19, 1), (38, 1), (47, 1), (61, 1), (62, 1), (63, 1), (64, 1)]\ndoes your service support multiple topics on a single kafka cluster\n[(0, 1), (2, 1), (59, 1), (60, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1)]\n\n\n how is this priced \n [(5, 1), (8, 1), (41, 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "sentences=cleaned_sentences_with_stopwords\n",
    "#sentences=cleaned_sentences\n",
    "\n",
    "# Split it by white space \n",
    "sentence_words = [[word for word in document.split() ]\n",
    "         for document in sentences]\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(sentence_words)\n",
    "for key, value in dictionary.items():\n",
    "    print(key, ' : ', value)\n",
    "\n",
    "import pprint\n",
    "# bow_corpus = [dictionary.doc2bow(text) for text in sentence_words]\n",
    "# for sent,embedding in zip(sentences,bow_corpus):\n",
    "#     print(sent)\n",
    "#     print(embedding)\n",
    "\n",
    "question_orig=\"how is this priced\"\n",
    "question=clean_sentence(question_orig,stopwords=False);\n",
    "# question_embedding = dictionary.doc2bow(question.split())\n",
    "\n",
    "# print(\"\\n\\n\",question,\"\\n\",question_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.19611613513818404 how to identify if a cluster is unhealthy what are the first steps or metrics we should look for\n1 0.19611613513818404 what happens to the quotas when there is a broker failure\n2 0.994309153919809 document the process to open firewall from cloud to onprem or vice versa\n3 0.19611613513818404 how to add wildcard access to a specific topic in a cluster\n4 1.0000000000000002 how is this billed\n5 0.9920614219374349 can i change the retention of my pipeline\n6 0.19611613513818404 why is creating a pipeline with topic affinity taking so long\n7 0.9849709602558829 can i scale up my pipeline\n8 0.19611613513818404 can i delete a single topic on a cluster\n9 0.19611613513818404 what is the difference between a pipeline a topic a broker a partition and a cluster\n10 0.19611613513818404 does your service support multiple topics on a single kafka cluster\n\n\nQuestion:  how is this priced\n\n\nRetrieved:  How is this billed?\n5\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity;\n",
    "def retrieveAndPrintFAQAnswer(question_embedding,sentence_embeddings,FAQdf,sentences):\n",
    "    max_sim=-1;\n",
    "    index_sim=-1;\n",
    "    for index,faq_embedding in enumerate(sentence_embeddings):\n",
    "        #sim=cosine_similarity(embedding.reshape(1, -1),question_embedding.reshape(1, -1))[0][0];\n",
    "        sim=cosine_similarity(faq_embedding,question_embedding)[0][0];\n",
    "        print(index, sim, sentences[index])\n",
    "        if sim>max_sim:\n",
    "            max_sim=sim;\n",
    "            index_sim=index;\n",
    "       \n",
    "    print(\"\\n\")\n",
    "    print(\"Question: \",question)\n",
    "    print(\"\\n\");\n",
    "    print(\"Retrieved: \",FAQdf.iloc[index_sim,0]) \n",
    "    print(FAQdf.iloc[index_sim,1])        \n",
    "    \n",
    "#retrieveAndPrintFAQAnswer(question_embedding,bow_corpus,df,sentences);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Embeddings : \n",
    "\n",
    "Glove is an alternate approach to build word embeddings using matrix factorization techinques on the word-word co-occurance matrix. \n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "While both the techniques are popular, glove performs better on some datasets while word2vec skipgram model performs better on some. Here, we experiment with both the word2vec and the glove models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded glove model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec \n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "glove_model=None;\n",
    "try:\n",
    "    glove_model = gensim.models.KeyedVectors.load(\"./glovemodel.mod\")\n",
    "    print(\"Loaded glove model\")\n",
    "except:            \n",
    "    glove_model = api.load('glove-twitter-25')\n",
    "    glove_model.save(\"./glovemodel.mod\")\n",
    "    print(\"Saved glove model\")\n",
    "    \n",
    "glove_embedding_size=len(glove_model['computer']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordVec(word,model):\n",
    "        samp=model['computer'];\n",
    "        vec=[0]*len(samp);\n",
    "        try:\n",
    "                vec=model[word];\n",
    "        except:\n",
    "                vec=[0]*len(samp);\n",
    "        return (vec)\n",
    "\n",
    "\n",
    "def getPhraseEmbedding(phrase,embeddingmodel):\n",
    "                       \n",
    "        samp=getWordVec('computer', embeddingmodel);\n",
    "        vec=numpy.array([0]*len(samp));\n",
    "        den=0;\n",
    "        for word in phrase.split():\n",
    "            #print(word)\n",
    "            den=den+1;\n",
    "            vec=vec+numpy.array(getWordVec(word,embeddingmodel));\n",
    "        #vec=vec/den;\n",
    "        #return (vec.tolist());\n",
    "        return vec.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.8349929770635101 how to identify if a cluster is unhealthy what are the first steps or metrics we should look for\n1 0.7686536998263341 what happens to the quotas when there is a broker failure\n2 0.7840371073746832 document the process to open firewall from cloud to onprem or vice versa\n3 0.8228934099586328 how to add wildcard access to a specific topic in a cluster\n4 0.2619826370675414 how is this billed\n5 0.7020266814466231 can i change the retention of my pipeline\n6 0.879914780346787 why is creating a pipeline with topic affinity taking so long\n7 0.5647586598382187 can i scale up my pipeline\n8 0.8215086809402338 can i delete a single topic on a cluster\n9 0.7387740618133274 what is the difference between a pipeline a topic a broker a partition and a cluster\n10 0.8504574561672683 does your service support multiple topics on a single kafka cluster\n\n\nQuestion:  how is this priced\n\n\nRetrieved:  Why is creating a pipeline with topic Affinity taking so long?\n7\n"
     ]
    }
   ],
   "source": [
    "#With Glove\n",
    "\n",
    "sent_embeddings=[];\n",
    "for sent in cleaned_sentences:\n",
    "    sent_embeddings.append(getPhraseEmbedding(sent,glove_model));\n",
    "    \n",
    "question_embedding=getPhraseEmbedding(question,glove_model);\n",
    "\n",
    "retrieveAndPrintFAQAnswer(question_embedding,sent_embeddings,df, cleaned_sentences);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "e404b59586357c814bc0d3940e75d6763c00a48753b225b81f7716971b8e1741"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}